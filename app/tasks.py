"""
BookingScraper/app/tasks.py  (fix SQL injection v2.1 + images_count v2.2)
Tareas Celery para scraping asÃ­ncrono - BookingScraper Pro
Windows 11 + Python 3.14.3

CORRECCIONES v2.2:
  [FIX CRITICO] images_count: columna no estaba en el INSERT â†’ siempre 0.
                Ahora se calcula con len(images_urls) y se guarda en BD.
  [FIX] Descarga de imÃ¡genes: eliminado lÃ­mite artificial [:30]. Se descargan TODAS.

CORRECCIONES v1.1:
  [FIX] Columna url_id en INSERT hotels (era url_queue_id en models anterior)
  [FIX] scraped_at: columna ahora existe en URLQueue (aÃ±adida en models.py)
  [FIX] review_scores, services, facilities, rooms_info â†’ JSON real (no str/join)
  [FIX] images_urls â†’ JSON real (no join con coma)
  [FIX] text() en todos los raw SQL (SQLAlchemy 2.0)
  [NEW] save_system_metrics(): tarea periÃ³dica de mÃ©tricas del sistema
  [NEW] cleanup_old_logs(): limpieza de logs antiguos (referenciada en beat)
  [FIX] Imports desde app.celery_app (no redefinir Celery aquÃ­)
"""

import json
import time
import psutil
from datetime import datetime, timedelta

from sqlalchemy import text
from loguru import logger

from app.celery_app import celery_app
from app.database import SessionLocal
from app.config import settings


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAREA PRINCIPAL: SCRAPING DE UN HOTEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(bind=True, max_retries=3, name="app.tasks.scrape_hotel_task")
def scrape_hotel_task(self, url_id: int):
    """
    Scrapea un hotel completo en todos los idiomas habilitados.

    Args:
        url_id: ID de la URL en la tabla url_queue
    """
    db = SessionLocal()
    start_time = time.time()

    try:
        # â”€â”€ 1. Obtener URL de la BD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        row = db.execute(
            text("SELECT url, language FROM url_queue WHERE id = :id"),
            {"id": url_id}
        ).fetchone()

        if not row:
            logger.error(f"URL ID {url_id} no encontrada en url_queue")
            return {"error": "URL no encontrada"}

        base_url = row[0]
        base_lang = row[1] or "en"

        logger.info(f"ğŸ”„ Procesando URL ID {url_id}: {base_url}")

        # â”€â”€ 2. Marcar como 'processing' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        db.execute(
            text("""
                UPDATE url_queue
                SET status = 'processing', updated_at = NOW()
                WHERE id = :id
            """),
            {"id": url_id}
        )
        db.commit()

        # â”€â”€ 3. Scraping por idioma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        from app.scraper import BookingScraper
        from app.image_downloader import ImageDownloader

        languages = settings.ENABLED_LANGUAGES  # propiedad que parsea el CSV
        scraped_count = 0
        first_hotel_name = None

        for lang in languages:
            try:
                # Construir URL con extensiÃ³n de idioma
                ext = settings.LANGUAGE_EXT.get(lang, f".{lang}")
                if ext:
                    # Reemplazar .html por .{lang}.html
                    lang_url = base_url.replace(".html", f"{ext}.html")
                else:
                    lang_url = base_url

                logger.info(f"  â†’ Idioma {lang}: {lang_url}")

                # Crear scraper y extraer
                with BookingScraper() as scraper:
                    data = scraper.scrape_hotel(lang_url, language=lang)

                if not data or not data.get("name"):
                    logger.warning(f"  âš ï¸ Sin datos para {lang}")
                    _log_scraping(db, url_id, lang, "no_data",
                                  time.time() - start_time, 0, "No se extrajeron datos")
                    continue

                if first_hotel_name is None:
                    first_hotel_name = data.get("name")

                # â”€â”€ 4. Guardar hotel en BD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # âœ… FIX: url_id (no url_queue_id); JSON real con json.dumps()
                db.execute(
                    text("""
                        INSERT INTO hotels (
                            url_id, url, language,
                            name, address, description,
                            rating, total_reviews, rating_category,
                            review_scores, services, facilities,
                            house_rules, important_info, rooms_info,
                            images_urls, images_count, scraped_at, updated_at
                        ) VALUES (
                            :url_id, :url, :language,
                            :name, :address, :description,
                            :rating, :total_reviews, :rating_category,
                            :review_scores::jsonb, :services::jsonb, :facilities::jsonb,
                            :house_rules, :important_info, :rooms_info::jsonb,
                            :images_urls::jsonb, :images_count, NOW(), NOW()
                        )
                        ON CONFLICT (url_id, language) DO UPDATE SET
                            name            = EXCLUDED.name,
                            address         = EXCLUDED.address,
                            description     = EXCLUDED.description,
                            rating          = EXCLUDED.rating,
                            total_reviews   = EXCLUDED.total_reviews,
                            rating_category = EXCLUDED.rating_category,
                            review_scores   = EXCLUDED.review_scores,
                            services        = EXCLUDED.services,
                            facilities      = EXCLUDED.facilities,
                            house_rules     = EXCLUDED.house_rules,
                            important_info  = EXCLUDED.important_info,
                            rooms_info      = EXCLUDED.rooms_info,
                            images_urls     = EXCLUDED.images_urls,
                            images_count    = EXCLUDED.images_count,
                            updated_at      = NOW()
                    """),
                    {
                        "url_id":           url_id,
                        "url":              lang_url,
                        "language":         lang,
                        "name":             data.get("name"),
                        "address":          data.get("address"),
                        "description":      data.get("description"),
                        "rating":           data.get("rating"),
                        "total_reviews":    data.get("total_reviews"),
                        "rating_category":  data.get("rating_category"),
                        # âœ… FIX: json.dumps() â†’ JSON real, no str()
                        "review_scores":    json.dumps(data.get("review_scores") or {}),
                        "services":         json.dumps(data.get("services")      or []),
                        "facilities":       json.dumps(data.get("facilities")    or {}),
                        "house_rules":      data.get("house_rules"),
                        "important_info":   data.get("important_info"),
                        "rooms_info":       json.dumps(data.get("rooms")         or []),
                        # âœ… FIX: lista de URLs como JSON array, no join con coma
                        "images_urls":      json.dumps(data.get("images_urls")   or []),
                        # âœ… FIX v2.2: images_count calculado desde la lista real
                        "images_count":     len(data.get("images_urls")          or []),
                    }
                )
                db.commit()

                scraped_count += 1
                duration = time.time() - start_time
                _log_scraping(db, url_id, lang, "completed", duration,
                              len(data.get("images_urls") or []))

                # â”€â”€ 5. Descargar imÃ¡genes (solo en idioma base) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if lang == base_lang and settings.DOWNLOAD_IMAGES:
                    img_urls = data.get("images_urls") or []
                    if img_urls:
                        try:
                            downloader = ImageDownloader()
                            # [FIX v2.2] Sin lÃ­mite artificial: descargar TODAS las imÃ¡genes
                            downloader.download_images(url_id, img_urls)
                            logger.info(f"  ğŸ“· {len(img_urls)} imÃ¡genes descargadas")
                        except Exception as img_err:
                            logger.warning(f"  âš ï¸ Error imÃ¡genes: {img_err}")

            except Exception as lang_err:
                logger.error(f"  âœ— Error en idioma {lang}: {lang_err}")
                _log_scraping(db, url_id, lang, "error",
                              time.time() - start_time, 0, str(lang_err))

        # â”€â”€ 6. Actualizar URL queue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        new_status = "completed" if scraped_count > 0 else "failed"
        # âœ… FIX: scraped_at ahora existe en URLQueue (aÃ±adida en models.py)
        db.execute(
            text("""
                UPDATE url_queue
                SET status = :status, scraped_at = NOW(), updated_at = NOW()
                WHERE id = :id
            """),
            {"status": new_status, "id": url_id}
        )
        db.commit()

        total_duration = time.time() - start_time
        logger.success(
            f"âœ… {url_id} â†’ {new_status} | {scraped_count}/{len(languages)} idiomas "
            f"| {total_duration:.1f}s | {first_hotel_name}"
        )

        return {
            "success":    scraped_count > 0,
            "hotel_name": first_hotel_name,
            "languages":  scraped_count,
            "duration":   round(total_duration, 2),
        }

    except Exception as e:
        logger.error(f"âŒ Error fatal en task URL {url_id}: {e}")
        db.rollback()

        # Marcar como fallida y registrar error
        try:
            db.execute(
                text("""
                    UPDATE url_queue
                    SET status = CASE
                            WHEN retry_count + 1 >= max_retries THEN 'failed'
                            ELSE 'pending'
                        END,
                        retry_count = retry_count + 1,
                        last_error  = :error,
                        updated_at  = NOW()
                    WHERE id = :id
                """),
                {"id": url_id, "error": str(e)[:500]}
            )
            db.commit()
        except Exception:
            pass

        # Reintentar si no superÃ³ el mÃ¡ximo
        if self.request.retries < self.max_retries:
            logger.info(f"ğŸ”„ Reintentando URL {url_id} (intento {self.request.retries + 1})")
            raise self.retry(exc=e, countdown=settings.RETRY_DELAY)

        return {"error": str(e)}

    finally:
        db.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAREA DE DESPACHO: BATCH DE URLs PENDIENTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(name="app.tasks.process_pending_urls")
def process_pending_urls(batch_size: int = 5):
    """
    Despacha un lote de URLs pendientes al worker.
    Ejecutada periÃ³dicamente por Celery Beat (cada 30s).

    Args:
        batch_size: NÃºmero mÃ¡ximo de URLs a despachar por ciclo
    """
    db = SessionLocal()
    try:
        rows = db.execute(
            text("""
                SELECT id FROM url_queue
                WHERE status = 'pending'
                  AND retry_count < max_retries
                ORDER BY priority DESC, created_at ASC
                LIMIT :limit
            """),
            {"limit": batch_size}
        ).fetchall()

        url_ids = [r[0] for r in rows]

        if not url_ids:
            logger.debug("â„¹ï¸ No hay URLs pendientes")
            return {"dispatched": 0}

        for uid in url_ids:
            scrape_hotel_task.delay(uid)

        logger.info(f"ğŸš€ Despachadas {len(url_ids)} tareas de scraping")
        return {"dispatched": len(url_ids), "url_ids": url_ids}

    finally:
        db.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAREA PERIÃ“DICA: MÃ‰TRICAS DEL SISTEMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(name="app.tasks.save_system_metrics")
def save_system_metrics():
    """
    Captura y guarda mÃ©tricas del sistema cada 5 minutos.
    Referenciada en beat_schedule de celery_app.py.
    """
    db = SessionLocal()
    try:
        # Stats de URL queue
        stats = db.execute(text("""
            SELECT
                COUNT(*) FILTER (WHERE status = 'pending')    AS pending,
                COUNT(*) FILTER (WHERE status = 'processing') AS processing,
                COUNT(*) FILTER (WHERE status = 'completed')  AS completed,
                COUNT(*) FILTER (WHERE status = 'failed')     AS failed
            FROM url_queue
        """)).fetchone()

        hotels_total = db.execute(
            text("SELECT COUNT(*) FROM hotels")
        ).scalar() or 0

        images_total = db.execute(
            text("SELECT COALESCE(SUM(images_count), 0) FROM hotels")
        ).scalar() or 0

        # Recursos del sistema
        cpu    = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory().percent
        disk   = psutil.disk_usage("C:\\").percent if psutil.disk_usage.__doc__ else 0.0

        db.execute(
            text("""
                INSERT INTO system_metrics (
                    urls_pending, urls_processing, urls_completed, urls_failed,
                    hotels_scraped, images_downloaded,
                    cpu_usage, memory_usage, disk_usage,
                    recorded_at
                ) VALUES (
                    :pending, :processing, :completed, :failed,
                    :hotels, :images,
                    :cpu, :memory, :disk,
                    NOW()
                )
            """),
            {
                "pending":    stats[0] or 0,
                "processing": stats[1] or 0,
                "completed":  stats[2] or 0,
                "failed":     stats[3] or 0,
                "hotels":     hotels_total,
                "images":     images_total,
                "cpu":        cpu,
                "memory":     memory,
                "disk":       disk,
            }
        )
        db.commit()
        logger.debug(
            f"ğŸ“Š MÃ©tricas guardadas | CPU:{cpu}% MEM:{memory}% "
            f"pending:{stats[0]} completed:{stats[2]}"
        )
        return {"recorded": True}

    except Exception as e:
        logger.error(f"Error guardando mÃ©tricas: {e}")
        return {"error": str(e)}
    finally:
        db.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAREA PERIÃ“DICA: LIMPIEZA DE LOGS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@celery_app.task(name="app.tasks.cleanup_old_logs")
def cleanup_old_logs(days: int = 30):
    """
    Elimina logs de scraping con mÃ¡s de `days` dÃ­as de antigÃ¼edad.
    Referenciada en beat_schedule de celery_app.py.
    """
    db = SessionLocal()
    try:
        # [FIX v2.1] Usar bind parameter para evitar injection de SQL.
        # El pattern .replace() anterior era vulnerable si days llegara
        # de fuente externa. Aqui usamos el multiplicador nativo de INTERVAL.
        result = db.execute(
            text("""
                DELETE FROM scraping_logs
                WHERE timestamp < NOW() - (INTERVAL '1 day' * :days)
            """),
            {"days": days}
        )
        db.commit()
        deleted = result.rowcount
        logger.info(f"ğŸ§¹ Logs limpiados: {deleted} registros eliminados (>{days} dÃ­as)")
        return {"deleted": deleted}
    except Exception as e:
        logger.error(f"Error limpiando logs: {e}")
        return {"error": str(e)}
    finally:
        db.close()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UTILIDAD INTERNA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _log_scraping(
    db, url_id: int, language: str,
    status: str, duration: float,
    items: int, error: str = None
):
    """Inserta una lÃ­nea en scraping_logs."""
    try:
        db.execute(
            text("""
                INSERT INTO scraping_logs
                    (url_id, language, status, duration_seconds, items_extracted, error_message, timestamp)
                VALUES
                    (:url_id, :lang, :status, :dur, :items, :error, NOW())
            """),
            {
                "url_id": url_id,
                "lang":   language,
                "status": status,
                "dur":    round(duration, 2),
                "items":  items,
                "error":  error,
            }
        )
        db.commit()
    except Exception as e:
        logger.debug(f"No se pudo registrar log: {e}")
